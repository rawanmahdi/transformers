{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea1477-eb8f-47fd-81e7-8b802f6a91dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  2043k      0 --:--:-- --:--:-- --:--:-- 2039k\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "!curl https://raw.githubusercontent.com/karpathy/ng-video-lecture/refs/heads/master/input.txt -o input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607fbd49-3b98-4628-b8a6-0ecb8fd39deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in chars: 1115394\n",
      "first thousand chars: \n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "with open('../input_data', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in chars:\", len(text))\n",
    "print(\"first thousand chars: \\n\", text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d046ef8b-6e46-4cab-b165-039e919e0eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  65\n",
      "vocab:  \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# get all unique characters\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print('vocab size: ',vocab_size)\n",
    "print('vocab: ',''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fce9e99-0edc-4dab-ae21-c62a497f75ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 56, 39, 61, 39, 51, 47, 50, 63]\n",
      "my name is rawamily\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# tokenize the input text by characters\n",
    "\n",
    "str2int = { ch:i for i,ch in enumerate(vocab)}\n",
    "int2str = { i:ch for i,ch in enumerate(vocab)}\n",
    "\n",
    "encoder = lambda s: [str2int[c] for c in s] # given string, return list of ints \n",
    "decorder = lambda l: ''.join([int2str[i] for i in l]) # given list, return string\n",
    "\n",
    "print(encoder(\"my name is rawamily\"))\n",
    "print(decorder(encoder(\"my name is rawamily\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f443672c-6e5f-4de1-97b5-d21b3b137d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "# encode input dataset and place it in tensor \n",
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f012514-fd9b-44fd-8824-5811b91c7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# train validation split \n",
    "n = int(.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0dd9cc4-5b12-4a01-9b23-d1768bd8d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), the target is 47\n",
      "when input is tensor([18, 47]), the target is 56\n",
      "when input is tensor([18, 47, 56]), the target is 57\n",
      "when input is tensor([18, 47, 56, 57]), the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "block_size = 8\n",
    "train_data[:block_size+1]\n",
    "\n",
    "# illustration of prediction based on full context of the block:\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52c7c8c9-5fea-471e-8133-378887ac133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      " torch.Size([4, 8]) \n",
      " tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: \n",
      " torch.Size([4, 8]) \n",
      " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# batching for parallel processing of blocks \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # number of independent sequences processed in parallel\n",
    "block_size = 8 # maximum context length for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,)) # generate $batch_size number of random indexes\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # batch chunks for each random index \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # offset by 1 for next char prediction\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs: \\n', xb.shape, \"\\n\", xb)\n",
    "print('targets: \\n', yb.shape, \"\\n\", yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "034d6bad-69af-4b51-b0bf-23766b60c357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24])\n",
      "when the input is [24], the target: 43\n",
      "tensor([24, 43])\n",
      "when the input is [24, 43], the target: 58\n",
      "tensor([24, 43, 58])\n",
      "when the input is [24, 43, 58], the target: 5\n",
      "tensor([24, 43, 58,  5])\n",
      "when the input is [24, 43, 58, 5], the target: 57\n",
      "tensor([24, 43, 58,  5, 57])\n",
      "when the input is [24, 43, 58, 5, 57], the target: 1\n",
      "tensor([24, 43, 58,  5, 57,  1])\n",
      "when the input is [24, 43, 58, 5, 57, 1], the target: 46\n",
      "tensor([24, 43, 58,  5, 57,  1, 46])\n",
      "when the input is [24, 43, 58, 5, 57, 1, 46], the target: 43\n",
      "tensor([24, 43, 58,  5, 57,  1, 46, 43])\n",
      "when the input is [24, 43, 58, 5, 57, 1, 46, 43], the target: 39\n",
      "tensor([44])\n",
      "when the input is [44], the target: 53\n",
      "tensor([44, 53])\n",
      "when the input is [44, 53], the target: 56\n",
      "tensor([44, 53, 56])\n",
      "when the input is [44, 53, 56], the target: 1\n",
      "tensor([44, 53, 56,  1])\n",
      "when the input is [44, 53, 56, 1], the target: 58\n",
      "tensor([44, 53, 56,  1, 58])\n",
      "when the input is [44, 53, 56, 1, 58], the target: 46\n",
      "tensor([44, 53, 56,  1, 58, 46])\n",
      "when the input is [44, 53, 56, 1, 58, 46], the target: 39\n",
      "tensor([44, 53, 56,  1, 58, 46, 39])\n",
      "when the input is [44, 53, 56, 1, 58, 46, 39], the target: 58\n",
      "tensor([44, 53, 56,  1, 58, 46, 39, 58])\n",
      "when the input is [44, 53, 56, 1, 58, 46, 39, 58], the target: 1\n",
      "tensor([52])\n",
      "when the input is [52], the target: 58\n",
      "tensor([52, 58])\n",
      "when the input is [52, 58], the target: 1\n",
      "tensor([52, 58,  1])\n",
      "when the input is [52, 58, 1], the target: 58\n",
      "tensor([52, 58,  1, 58])\n",
      "when the input is [52, 58, 1, 58], the target: 46\n",
      "tensor([52, 58,  1, 58, 46])\n",
      "when the input is [52, 58, 1, 58, 46], the target: 39\n",
      "tensor([52, 58,  1, 58, 46, 39])\n",
      "when the input is [52, 58, 1, 58, 46, 39], the target: 58\n",
      "tensor([52, 58,  1, 58, 46, 39, 58])\n",
      "when the input is [52, 58, 1, 58, 46, 39, 58], the target: 1\n",
      "tensor([52, 58,  1, 58, 46, 39, 58,  1])\n",
      "when the input is [52, 58, 1, 58, 46, 39, 58, 1], the target: 46\n",
      "tensor([25])\n",
      "when the input is [25], the target: 17\n",
      "tensor([25, 17])\n",
      "when the input is [25, 17], the target: 27\n",
      "tensor([25, 17, 27])\n",
      "when the input is [25, 17, 27], the target: 10\n",
      "tensor([25, 17, 27, 10])\n",
      "when the input is [25, 17, 27, 10], the target: 0\n",
      "tensor([25, 17, 27, 10,  0])\n",
      "when the input is [25, 17, 27, 10, 0], the target: 21\n",
      "tensor([25, 17, 27, 10,  0, 21])\n",
      "when the input is [25, 17, 27, 10, 0, 21], the target: 1\n",
      "tensor([25, 17, 27, 10,  0, 21,  1])\n",
      "when the input is [25, 17, 27, 10, 0, 21, 1], the target: 54\n",
      "tensor([25, 17, 27, 10,  0, 21,  1, 54])\n",
      "when the input is [25, 17, 27, 10, 0, 21, 1, 54], the target: 39\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "# inputs vs targets illustration\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        print(context)\n",
    "        target = yb[b, t]\n",
    "        print(f\"when the input is {context.tolist()}, the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining our torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a00863-ff8b-4a61-8cc3-542e872b4f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None: # in the case where we are running inference\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # convert array to 2D\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target=targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices for the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # get last time step, (B,C)\n",
    "            probs = F.softmax(logits, dim=1) # get probability \n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decorder(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training using an Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a952c3ef-b8bc-43d3-b8b1-341db905d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# get pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11238c2a-7fe2-4ac0-8337-5defa3ca6930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 0: 2.4636545181274414\n",
      "loss at step 1000: 2.390366792678833\n",
      "loss at step 2000: 2.5757007598876953\n",
      "loss at step 3000: 2.584615468978882\n",
      "loss at step 4000: 2.4348089694976807\n",
      "loss at step 5000: 2.449105739593506\n",
      "loss at step 6000: 2.4397647380828857\n",
      "loss at step 7000: 2.4875221252441406\n",
      "loss at step 8000: 2.3294029235839844\n",
      "loss at step 9000: 2.418365716934204\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "# training loop\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # sample a batch\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 1000 == 0:\n",
    "\n",
    "        print(f\"loss at step {steps}: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c83ef7c-a2f6-4a2a-9273-446f037f9276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ofows ht IUS:\n",
      "S:\n",
      "\n",
      "ING flvenje ssutefr,\n",
      "M:\n",
      "War cl igagimous pray whars:\n",
      "Panalit I It aithit terised thevermenghau buaror VOubed spo mng as chathab llll:\n",
      "Ware,\n",
      "\n",
      "ee her,\n",
      "Thooured aly y hindr's.\n",
      "Fashat--\n",
      "MNGes s, share hathure Anfaneof f s llon!\n",
      "\n",
      "ICLiroushange\n",
      "\n",
      "Then\n",
      "Magend cugss, be jollrty\n",
      "\n",
      "AROUFLom, ifay wil wher, gheatalloult llats howheprshakengayoref f f abighine ck orors n s?\n",
      "NGABerd Foutheig vemy.\n",
      "NG t isoststor hnor 'myougorme whe s'car n r toun t pridie are he of t ad\n",
      "BY:\n",
      "Hatamethat vint i\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "print(decorder(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical trick of self-attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# information should flow from past context to the current idx\n",
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bag of words which will hold averaging using previous context\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      " tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      " tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# use matrix multiplication for efficiency \n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print('a=\\n', a)\n",
    "print('b=\\n', b)\n",
    "print('c=\\n', c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to the attention average mechanism \n",
    "wei = torch.tril(torch.ones(T,T)) # weighted aggregation/weighted sum. the triangular shape allows us to only pull context from the past\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # size (B,T,T) multiplied by (B,T,C) results in (B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using softmax\n",
    "from torch.nn import functional as F\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # tokens from the future will not be aggregated by setting them to -inf and taking softmax\n",
    "wei = F.softmax(wei, dim=-1) # normalize\n",
    "xbow3 = wei @ x # weighted aggregation\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T)) \n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # tokens from the future will not be aggregated by setting them to -inf and taking softmax\n",
    "wei = F.softmax(wei, dim=-1) # normalize\n",
    "out = wei @ x # weighted aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every single token will produce 2 vectors \n",
    "# query vector: what information am I looking for from previous tokens?\n",
    "# key vector: what information do I contain?\n",
    "# affinity = dot product of query and key vector \n",
    "\n",
    "# create a single head of self-attention \n",
    "head_size = 16\n",
    "key = torch.nn.Linear(C, head_size, bias=False)\n",
    "query = torch.nn.Linear(C, head_size, bias=False)\n",
    "value = torch.nn.Linear(C, head_size, bias=False)\n",
    "# all tokens in x produce a key and a query \n",
    "k = key(x) # size B,T,16\n",
    "q = query(x) # size B,T,16\n",
    "v = value(x)\n",
    "wei = q @ k.transpose(-2, -1) # B,T,16 multiplies B,16,T to produce B,T,T\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # tokens from the future will not be aggregated by setting them to -inf and taking softmax\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v # weighted aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.9456, 0.0544, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8952, 0.0486, 0.0562, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0223, 0.0651, 0.1234, 0.7892, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0711, 0.0019, 0.0034, 0.0080, 0.9155, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.1353, 0.2752, 0.0972, 0.4712, 0.0141, 0.0000, 0.0000],\n",
       "         [0.1561, 0.1033, 0.1465, 0.0880, 0.0698, 0.3634, 0.0728, 0.0000],\n",
       "         [0.4031, 0.0104, 0.0134, 0.0060, 0.5049, 0.0365, 0.0194, 0.0064]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0180, 0.9820, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3105, 0.2458, 0.4437, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0730, 0.3275, 0.2227, 0.3769, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1411, 0.5030, 0.0321, 0.2000, 0.1238, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0613, 0.5558, 0.0628, 0.2372, 0.0036, 0.0792, 0.0000, 0.0000],\n",
       "         [0.1382, 0.0249, 0.0869, 0.3543, 0.3057, 0.0477, 0.0422, 0.0000],\n",
       "         [0.0325, 0.1243, 0.2556, 0.3944, 0.0374, 0.0160, 0.1121, 0.0277]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7713, 0.2287, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3325, 0.5330, 0.1345, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0565, 0.4092, 0.5132, 0.0210, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5608, 0.0088, 0.0167, 0.0827, 0.3309, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4997, 0.2308, 0.0601, 0.1110, 0.0079, 0.0907, 0.0000, 0.0000],\n",
       "         [0.0098, 0.1964, 0.1426, 0.3822, 0.0275, 0.0472, 0.1942, 0.0000],\n",
       "         [0.0443, 0.1419, 0.1491, 0.0331, 0.3913, 0.0647, 0.0413, 0.1343]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7558, 0.2442, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.9079, 0.0135, 0.0786, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4163, 0.0873, 0.3233, 0.1730, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0320, 0.0222, 0.1247, 0.6382, 0.1829, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1075, 0.0722, 0.1816, 0.5429, 0.0394, 0.0563, 0.0000, 0.0000],\n",
       "         [0.2617, 0.0524, 0.2288, 0.0301, 0.1793, 0.1964, 0.0512, 0.0000],\n",
       "         [0.1406, 0.0203, 0.3022, 0.0705, 0.0257, 0.1814, 0.0141, 0.2453]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700, -0.3596,  ..., -0.8016,  1.5236,  2.5086],\n",
       "         [ 0.1349, -0.0798, -0.2852,  ..., -0.6747,  1.5283,  2.3503],\n",
       "         [ 0.0827, -0.0413, -0.2757,  ..., -0.6677,  1.3859,  2.2030],\n",
       "         ...,\n",
       "         [-1.4614,  0.8593,  0.7174,  ..., -0.1654,  0.6562, -0.6048],\n",
       "         [-0.7645,  0.0571,  0.2274,  ...,  0.7557,  0.4655,  0.5263],\n",
       "         [-1.0257,  0.8860,  0.5548,  ..., -0.5685,  1.4220,  0.6832]],\n",
       "\n",
       "        [[ 0.4562, -1.0917, -0.8207,  ...,  0.0512, -0.6576, -2.5729],\n",
       "         [ 0.0288,  0.9683, -1.2415,  ...,  0.7727, -1.1413,  1.2434],\n",
       "         [ 1.1232, -0.1891, -0.3211,  ..., -0.0769, -0.0063, -0.8308],\n",
       "         ...,\n",
       "         [ 0.2722,  0.8824, -0.5814,  ...,  0.5520, -0.6591,  0.6601],\n",
       "         [ 0.4160,  0.3460,  0.5326,  ...,  0.1628, -0.0332, -0.0418],\n",
       "         [ 0.7334,  0.5238,  0.5093,  ..., -0.0491,  0.1059,  0.1174]],\n",
       "\n",
       "        [[-0.6067,  1.8328,  0.2931,  ...,  1.0041,  0.8656,  0.1688],\n",
       "         [-0.5217,  1.3545,  0.2291,  ...,  0.9274,  0.8400,  0.0076],\n",
       "         [-0.4653,  0.5434, -0.0026,  ...,  0.8345,  0.7553, -0.2951],\n",
       "         ...,\n",
       "         [-0.3495,  0.5919,  0.1867,  ...,  0.6047,  0.5515, -0.0214],\n",
       "         [ 0.4340, -0.5658,  0.3699,  ...,  0.5395,  0.2493,  0.0466],\n",
       "         [-0.4310,  0.2792,  0.5437,  ...,  0.3093, -0.4159,  0.1305]],\n",
       "\n",
       "        [[ 0.3330,  1.0995,  0.4034,  ...,  1.6634, -0.4718,  0.5857],\n",
       "         [ 0.0178,  1.0614, -0.2321,  ...,  1.0791, -0.3163,  0.3599],\n",
       "         [ 0.4105,  1.0914,  0.4417,  ...,  1.4970, -0.4458,  0.4683],\n",
       "         ...,\n",
       "         [ 0.5597,  0.6186,  0.4447,  ..., -1.2294,  0.6687, -0.5823],\n",
       "         [ 0.7057,  0.4738,  0.4803,  ...,  0.8198,  0.4784,  0.1202],\n",
       "         [ 0.9071,  0.6791,  0.8214,  ...,  0.5839,  0.2198,  0.5705]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes on the self attention block\n",
    "- attention is a communication mechanism, can be seen as a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. \n",
    "- no notion of space, as opposed to convolutions, attention acts over a set of vectors. positional encodings add in that information\n",
    "- examples across batch dimensions are processed independently and never communicate  \n",
    "- the reason for not allowing communication between current tokens and future tokens is because of the language generation use case, if this were sentiment analysis, we could allow all tokens to communicate with all other tokens to capture full meaning. in this use case the next token prediction can only depend on past tokens, so future tokens are masked out in the `wei = wei.masked_fill(tril == 0, float('-inf'))` step\n",
    "- in the 'Attention is All You Need' paper, the following is the formula:\n",
    "### Attention(Q, K, V) = softmax((Q*K.transpose)/sqrt(head_size))*V\n",
    "which we have implemented, however its missing a normalization. \"Scaled\" attention additionally divides wei by 1/sqrt(head_size), makes it so when input Q, K (query and key) are unit variance, wei will be unit variance too and Softmax will stay diffused and not saturate too much:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0104)\n",
      "tensor(1.0204)\n",
      "tensor(17.6841)\n",
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "scaled means the variance goes down to 1:  tensor(1.1053)\n",
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n"
     ]
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "print(k.var())\n",
    "print(q.var())\n",
    "print(wei.var())\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "print(\"scaled means the variance goes down to 1: \", wei.var())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low variance softmax:  tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "high variance softmax:  tensor([0.0228, 0.0015, 0.1382, 0.0015, 0.8359])\n"
     ]
    }
   ],
   "source": [
    "# if wei has high variance, softmax will converge to one hot \n",
    "print(\"low variance softmax: \", torch.softmax(torch.tensor([.1, -.2, .3, -.2, .5]), dim=-1))\n",
    "print(\"high variance softmax: \", torch.softmax(torch.tensor([.1, -.2, .3, -.2, .5])*9, dim=-1)) \n",
    "# with hgih variance, softmax sharpens towards the highest value, and every token will then aggregate information from a single node, \n",
    "# the maximum, which is not what we want when trying to build context form a string of tokens "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
